{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwAAOwE5SlgbANsPR1GbW0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kaiser2110/MateriaIA/blob/main/Tarea6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGlOjkIUN7wa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_set = datasets.MNIST('/mnt/storage/Datasets', train=True, download=True, transform=transform)\n",
        "#test_set = datasets.MNIST('/mnt/storage/Datasets', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "#test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "        self.activation = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return nn.Sigmoid()(x)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(64, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 1024)\n",
        "        self.fc3 = nn.Linear(1024, 784)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return nn.Tanh()(x)\n",
        "    \n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "loss = nn.BCELoss()\n",
        "\n",
        "def noise(size):\n",
        "    n = torch.randn(size, 64)\n",
        "    return n.to(device)\n",
        "\n",
        "def discriminator_train_step(real_data, fake_data):\n",
        "    d_optimizer.zero_grad() #reiniciamos los gradientes\n",
        "    \n",
        "    #predecimos sobre los datos reales. Obtenemos la perdida y aplicamos backpropagation\n",
        "    prediction_real = discriminator(real_data)\n",
        "    #el valor esperado en este caso es 1 ya que son los datos reales\n",
        "    error_real = loss(prediction_real, torch.ones(len(real_data), 1).to(device))\n",
        "    error_real.backward()\n",
        "    \n",
        "    #predecimos sobre los datos falsos. Obtenemos la perdida y aplicamos backpropagation\n",
        "    prediction_fake = discriminator(fake_data)\n",
        "    #el valor esperado en este caso es 0 ya que son los datos falsos\n",
        "    error_fake = loss(prediction_fake, torch.zeros(len(fake_data), 1).to(device))\n",
        "    error_fake.backward()\n",
        "    #acutalizamos los pesos\n",
        "    d_optimizer.step()\n",
        "    #obtenemos la perdida total\n",
        "    return error_real + error_fake\n",
        "\n",
        "def generator_train_step(fake_data):\n",
        "    g_optimizer.zero_grad() #reiniciamos los gradientes\n",
        "    prediction = discriminator(fake_data) #predecimos sobre los datos falsos con el discriminador\n",
        "    #definimos el valor esperado como 1, ya que queremos los casos en los que el discriminador fall√≥ en diferenciar\n",
        "    error = loss(prediction, torch.ones(len(real_data), 1).to(device))  \n",
        "    error.backward()\n",
        "    g_optimizer.step()\n",
        "    return error\n",
        "\n",
        "d_losses = []\n",
        "g_losses = []\n",
        "\n",
        "for epoch in range(100):\n",
        "    N = len(train_loader)\n",
        "    d_loss_epoch = 0.0\n",
        "    g_loss_epoch = 0.0\n",
        "    for i, (images, _) in enumerate(train_loader):\n",
        "        real_data = images.view(len(images), -1).to(device)\n",
        "        fake_data = generator(noise(len(real_data))).to(device)\n",
        "        fake_data = fake_data.detach()\n",
        "        d_loss = discriminator_train_step(real_data, fake_data)\n",
        "        fake_data = generator(noise(len(real_data))).to(device)\n",
        "        g_loss = generator_train_step(fake_data)\n",
        "        d_loss_epoch += d_loss\n",
        "        g_loss_epoch += g_loss\n",
        "        if i % 100 == 0 or i == N:\n",
        "            print('[{}/{}, {}/{}] D_loss: {:.3f} G_loss: {:.3f}'\n",
        "                  .format(epoch, 100, i, len(train_loader), d_loss.item(), g_loss.item()))\n",
        "    d_losses.append(d_loss_epoch/N)\n",
        "    g_losses.append(g_loss_epoch/N)\n",
        "    print('Epoch {} D_loss_avg: {:.3f} G_loss_avg: {:.3f}'\n",
        "          .format(epoch, d_losses[-1], g_losses[-1]))\n",
        "    \n",
        "g_losses=[x.detach().numpy() for x in g_losses]\n",
        "d_losses=[x.detach().numpy() for x in d_losses]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(g_losses, label='Generator loss')\n",
        "plt.plot(d_losses, label='Discriminator Loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Define the transformation for preprocessing the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the CelebA dataset\n",
        "train_set = datasets.CelebA('/mnt/storage/Datasets', split='train', download=True, transform=transform)\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=4)\n",
        "\n",
        "# Discriminator model\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
        "        self.fc = nn.Linear(512 * 4 * 4, 1)\n",
        "        self.activation = nn.LeakyReLU(0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.conv1(x))\n",
        "        x = self.activation(self.conv2(x))\n",
        "        x = self.activation(self.conv3(x))\n",
        "        x = self.activation(self.conv4(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return nn.Sigmoid()(x)\n",
        "\n",
        "# Generator model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc = nn.Linear(100, 512 * 4 * 4)\n",
        "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)\n",
        "        self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.deconv4 = nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1)\n",
        "        self.activation = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(x.size(0), 512, 4, 4)\n",
        "        x = self.activation\n"
      ],
      "metadata": {
        "id": "R4OzfwEMTMHD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}